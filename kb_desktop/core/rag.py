from typing import List, Dict, Tuple, Optional
from core.llm import LLMClient
import re

# ç½®ä¿¡åº¦é˜ˆå€¼
LOW_CONFIDENCE_THRESHOLD = 0.3  # å¦‚æœ top-1 ç›¸ä¼¼åº¦ < è¿™ä¸ªå€¼ï¼Œè§¦å‘å¤‡ç”¨å›å¤
MIN_TOPK_VARIANCE = 0.05  # å¦‚æœæ‰€æœ‰ TopK åˆ†æ•°å¤ªç›¸ä¼¼ï¼Œå¯èƒ½æ˜¯å™ªå£°

class RAGGenerator:
    """
    RAG ç”Ÿæˆå™¨ï¼Œç»„è£…ä¸Šä¸‹æ–‡å¹¶ç”Ÿæˆå¸¦æœ‰å¼ºåˆ¶å¼•ç”¨çš„å›ç­”ã€‚
    """
    
    def __init__(self):
        self.llm = LLMClient()
    
    def check_confidence(self, context_chunks: List[Dict]) -> Tuple[bool, str]:
        """
        æ£€æŸ¥æ£€ç´¢ç»“æœæ˜¯å¦å…·æœ‰è¶³å¤Ÿçš„ç½®ä¿¡åº¦ã€‚
        
        Args:
            context_chunks: å¸¦æœ‰ 'similarity' åˆ†æ•°çš„æ£€ç´¢æ–‡æœ¬å—
            
        Returns:
            (is_confident: bool, reason: str)
        """
        if not context_chunks:
            return False, "æœªæ‰¾åˆ°ç›¸å…³æ–‡æ¡£"
        
        # æ£€æŸ¥ top-1 ç›¸ä¼¼åº¦
        top1_score = context_chunks[0].get('similarity', 0)
        if top1_score < LOW_CONFIDENCE_THRESHOLD:
            return False, f"æœ€ä½³åŒ¹é…ç›¸ä¼¼åº¦è¿‡ä½ ({top1_score:.3f} < {LOW_CONFIDENCE_THRESHOLD})"
        
        # æ£€æŸ¥ TopK ä¹‹é—´çš„æ–¹å·®ï¼ˆå®ƒä»¬æ˜¯å¦éƒ½å·®ä¸å¤šï¼Ÿï¼‰
        if len(context_chunks) >= 3:
            scores = [chunk.get('similarity', 0) for chunk in context_chunks]
            variance = max(scores) - min(scores)
            if variance < MIN_TOPK_VARIANCE:
                return False, f"æ‰€æœ‰ç»“æœåˆ†æ•°è¿‡ä½ä¸”ç›¸è¿‘ (æ–¹å·®: {variance:.3f})"
        
        return True, "ç½®ä¿¡åº¦è¶³å¤Ÿ"

    
    def generate_answer(self, query: str, context_chunks: List[Dict]) -> Tuple[str, List[Dict]]:
        """
        ç”Ÿæˆå¸¦æœ‰å¼ºåˆ¶å¼•ç”¨çš„å›ç­”ã€‚
        
        Args:
            query: ç”¨æˆ·çš„é—®é¢˜
            context_chunks: å­—å…¸åˆ—è¡¨ï¼ŒåŒ…å«é”®: 'text', 'filename', 'chunk_id', 'similarity'
            
        Returns:
            (answer_text, citations) çš„å…ƒç»„
            citations æ˜¯å­—å…¸åˆ—è¡¨: {'filename': str, 'chunk_id': int, 'excerpt': str}
        """
        # 1. ç”¨ä¸Šä¸‹æ–‡æ„å»ºæç¤º
        prompt = self._build_prompt(query, context_chunks)
        
        # 2. å‡†å¤‡æ¶ˆæ¯
        messages = [
            {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªçŸ¥è¯†åº“åŠ©æ‰‹ã€‚è¯·åŸºäºæä¾›çš„ä¸Šä¸‹æ–‡å›ç­”é—®é¢˜ï¼Œå¹¶åœ¨å›ç­”æœ«å°¾åˆ—å‡ºå¼•ç”¨æ¥æºã€‚"},
            {"role": "user", "content": prompt}
        ]
        
        # 3. è°ƒç”¨ LLMï¼ˆæµå¼ä¼ è¾“ï¼‰
        full_response = ""
        for chunk in self.llm.chat(messages, stream=True):
            full_response += chunk
        
        # 4. è§£æå¼•ç”¨ï¼ˆç®€å•æ–¹æ³•ï¼šä»å“åº”ä¸­æå–ï¼‰
        # å¯¹äº MVPï¼Œå¦‚æœ LLM æ²¡æœ‰æä¾›ï¼Œæˆ‘ä»¬å°†æ‰‹åŠ¨é™„åŠ å¼•ç”¨
        citations = self._extract_or_force_citations(full_response, context_chunks)
        
        return full_response, citations
    
    def _build_prompt(self, query: str, context_chunks: List[Dict]) -> str:
        """ç”¨ä¸Šä¸‹æ–‡æ„å»º RAG æç¤ºã€‚"""
        context_text = ""
        
        for i, chunk in enumerate(context_chunks):
            context_text += f"\nã€æ–‡æ¡£ {i+1}ã€‘æ¥æº: {chunk['filename']}\n"
            context_text += f"{chunk['text']}\n"
            context_text += "-" * 60 + "\n"
        
        prompt = f"""åŸºäºä»¥ä¸‹æ–‡æ¡£ç‰‡æ®µå›ç­”é—®é¢˜ã€‚è¯·åŠ¡å¿…åœ¨å›ç­”æœ«å°¾åˆ—å‡ºå¼•ç”¨çš„æ–‡æ¡£ç¼–å·ã€‚

å·²çŸ¥æ–‡æ¡£:
{context_text}

é—®é¢˜: {query}

è¦æ±‚:
1. ä»…åŸºäºä¸Šè¿°æ–‡æ¡£å†…å®¹å›ç­”
2. å¦‚æœæ–‡æ¡£ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·æ˜ç¡®è¯´æ˜
3. åœ¨å›ç­”æœ«å°¾åˆ—å‡ºå¼•ç”¨æ¥æºï¼Œæ ¼å¼: ã€å¼•ç”¨ã€‘æ–‡æ¡£1, æ–‡æ¡£2...
"""
        return prompt
    
    def _extract_or_force_citations(self, response: str, context_chunks: List[Dict]) -> List[Dict]:
        """
        ä»å“åº”ä¸­æå–å¼•ç”¨ï¼Œæˆ–å¦‚æœç¼ºå°‘åˆ™å¼ºåˆ¶æ·»åŠ ã€‚
        ç°åœ¨åŒ…æ‹¬å¼•ç”¨éªŒè¯ä»¥é˜²æ­¢å¹»è§‰ã€‚
        """
        citations = []
        
        # å°è¯•ä»å“åº”ä¸­æå–å¼•ç”¨çš„æ–‡æ¡£ç¼–å·
        # æŸ¥æ‰¾ç±»ä¼¼â€œæ–‡æ¡£1â€ã€â€œæ–‡æ¡£ 2â€ã€â€œ[1]â€ç­‰æ¨¡å¼
        cited_pattern = re.findall(r'(?:æ–‡æ¡£|[\[\(])(\d+)(?:[\]\)])?', response)
        cited_indices = set()
        
        for match in cited_pattern:
            try:
                idx = int(match) - 1  # Convert to 0-based
                if 0 <= idx < len(context_chunks):
                    cited_indices.add(idx)
            except ValueError:
                continue
        
        # å¦‚æœ LLM å¼•ç”¨äº†ç‰¹å®šæ–‡æœ¬å—ï¼ŒéªŒè¯å¹¶ä½¿ç”¨å®ƒä»¬
        if cited_indices:
            for idx in sorted(cited_indices):
                chunk = context_chunks[idx]
                citations.append({
                    'filename': chunk['filename'],
                    'chunk_id': chunk.get('chunk_id', idx),
                    'excerpt': chunk['text'][:100] + "..."
                })
        else:
            # å›é€€ï¼šå°†æ‰€æœ‰æä¾›çš„æ–‡æœ¬å—ä½œä¸ºæ½œåœ¨å¼•ç”¨åŒ…å«è¿›æ¥
            # ï¼ˆLLM åº”è¯¥å¼•ç”¨å®ƒä»¬ï¼Œä½†æ²¡æœ‰ - æ ‡è®°è¿™ä¸ªï¼‰
            for i, chunk in enumerate(context_chunks):
                citations.append({
                    'filename': chunk['filename'],
                    'chunk_id': chunk.get('chunk_id', i),
                    'excerpt': chunk['text'][:100] + "...",
                    'verified': False  # æ ‡è®°ä¸ºæœªéªŒè¯
                })
        
        return citations
    
    def verify_citations(self, response: str, context_chunks: List[Dict]) -> Tuple[bool, str]:
        """
        éªŒè¯å“åº”ä¸­çš„æ‰€æœ‰å¼•ç”¨æ˜¯å¦å¯¹åº”äºæ£€ç´¢çš„æ–‡æœ¬å—ã€‚
        
        Returns:
            (is_valid: bool, issue: str)
        """
        # ä»å“åº”ä¸­æå–å¼•ç”¨çš„ç´¢å¼•
        cited_pattern = re.findall(r'(?:æ–‡æ¡£|[\[\(])(\d+)(?:[\]\)])?', response)
        
        for match in cited_pattern:
            try:
                idx = int(match) - 1
                if idx < 0 or idx >= len(context_chunks):
                    return False, f"å¼•ç”¨æ–‡æ¡£{match}è¶…å‡ºèŒƒå›´ (æœ‰æ•ˆèŒƒå›´: 1-{len(context_chunks)})"
            except ValueError:
                continue
        
        return True, "å¼•ç”¨å·²éªŒè¯"


    def generate_fallback_response(self, query: str, context_chunks: List[Dict], reason: str) -> Tuple[str, List[Dict]]:
        """
        å½“ç½®ä¿¡åº¦å¤ªä½æ—¶ç”Ÿæˆæœ‰ç”¨çš„å¤‡ç”¨å›å¤ã€‚
        
        Args:
            query: ç”¨æˆ·çš„é—®é¢˜
            context_chunks: æ£€ç´¢çš„æ–‡æœ¬å—ï¼ˆå³ä½¿ç½®ä¿¡åº¦ä½ï¼‰
            reason: ä¸ºä»€ä¹ˆç½®ä¿¡åº¦ä½
            
        Returns:
            (fallback_message, empty_citations) çš„å…ƒç»„
        """
        # ä»æŸ¥è¯¢ä¸­æå–å…³é”®è¯ç”¨äºå»ºè®®
        keywords = self._extract_keywords(query)
        
        # æ„å»ºå¤‡ç”¨æ¶ˆæ¯
        fallback_msg = f"""âš ï¸ **çŸ¥è¯†åº“ç¼ºä¹è¶³å¤Ÿä¾æ®**

æŠ±æ­‰ï¼Œå½“å‰çŸ¥è¯†åº“ä¸­æ²¡æœ‰æ‰¾åˆ°è¶³å¤Ÿç›¸å…³çš„ä¿¡æ¯æ¥å›ç­”æ‚¨çš„é—®é¢˜ã€‚

**åŸå› **: {reason}

**å»ºè®®çš„è¿½é—®æ–¹å‘**:
"""
        
        # æ ¹æ®å…³é”®è¯ç”Ÿæˆåç»­é—®é¢˜
        if keywords:
            fallback_msg += f"\n1. å…³äº '{keywords[0]}' çš„å…·ä½“å®šä¹‰æˆ–èƒŒæ™¯æ˜¯ä»€ä¹ˆï¼Ÿ"
            if len(keywords) > 1:
                fallback_msg += f"\n2. '{keywords[1]}' åœ¨å“ªäº›åœºæ™¯ä¸‹é€‚ç”¨ï¼Ÿ"
            fallback_msg += f"\n3. èƒ½å¦æä¾›æ›´å…·ä½“çš„åœºæ™¯æˆ–æ¡ˆä¾‹ï¼Ÿ"
        else:
            fallback_msg += "\n1. èƒ½å¦æä¾›æ›´å…·ä½“çš„å…³é”®è¯æˆ–èƒŒæ™¯ä¿¡æ¯ï¼Ÿ"
            fallback_msg += "\n2. æ‚¨æƒ³äº†è§£çš„æ˜¯å“ªæ–¹é¢çš„å†…å®¹ï¼Ÿ"
            fallback_msg += "\n3. æ˜¯å¦å¯ä»¥æ¢ä¸ªæ–¹å¼æè¿°æ‚¨çš„é—®é¢˜ï¼Ÿ"
        
        # æ˜¾ç¤ºå¯ç”¨æ–‡æ¡£ä½œä¸ºæ¨è
        if context_chunks:
            fallback_msg += "\n\n**å¯èƒ½ç›¸å…³çš„æ–‡æ¡£** (ç›¸ä¼¼åº¦è¾ƒä½ï¼Œä»…ä¾›å‚è€ƒ):\n"
            for i, chunk in enumerate(context_chunks[:3]):  # Top 3
                fallback_msg += f"\n- {chunk['filename']} (ç›¸ä¼¼åº¦: {chunk.get('similarity', 0):.3f})"
        
        fallback_msg += "\n\nğŸ’¡ **æç¤º**: æ‚¨å¯ä»¥å°è¯•å¯¼å…¥æ›´å¤šç›¸å…³æ–‡æ¡£ï¼Œæˆ–ä½¿ç”¨ä¸åŒçš„å…³é”®è¯é‡æ–°æé—®ã€‚"
        
        return fallback_msg, []
    
    def _extract_keywords(self, query: str) -> List[str]:
        """ä»æŸ¥è¯¢ä¸­æå–ç®€å•å…³é”®è¯ï¼ˆç§»é™¤å¸¸è§è¯ï¼‰ã€‚"""
        # ç®€å•æ–¹æ³•ï¼šåˆ†å‰²å¹¶è¿‡æ»¤
        common_words = {'çš„', 'äº†', 'æ˜¯', 'åœ¨', 'æœ‰', 'å’Œ', 'å°±', 'ä¸', 'äºº', 'éƒ½', 'æˆ‘', 'è¦', 
                       'ä»–', 'è¿™', 'ç€', 'ä½ ', 'ä¼š', 'åœ°', 'ä¸ª', 'å¥¹', 'åˆ°', 'è¯´', 'ä»¬', 'ä¸º',
                       'ä»€ä¹ˆ', 'æ€ä¹ˆ', 'å¦‚ä½•', 'èƒ½å¦', 'å¯ä»¥', 'å—', 'å‘¢', 'ï¼Ÿ', '?'}
        
        words = re.findall(r'[\u4e00-\u9fa5]+', query)  # æå–ä¸­æ–‡è¯
        keywords = [w for w in words if w not in common_words and len(w) > 1]
        
        return keywords[:3]  # è¿”å›å‰3ä¸ªå…³é”®è¯

